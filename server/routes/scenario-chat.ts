import { Router } from 'express';
import { GoogleGenAI, Type } from '@google/genai';

const router = Router();
const GEMINI_API_KEY = process.env.GEMINI_API_KEY;
if (!GEMINI_API_KEY) {
  throw new Error('GEMINI_API_KEY environment variable is required. Please set it in your .env file.');
}
const ai = new GoogleGenAI({ apiKey: GEMINI_API_KEY });

/**
 * POST /api/scenario/start
 * å¼€å§‹ä¸€ä¸ªæ–°çš„å®æ—¶å¯¹è¯åœºæ™¯
 * 
 * Body: {
 *   topic: string,  // åœºæ™¯ä¸»é¢˜ï¼Œå¦‚ "buying apples at market"
 *   setting?: string  // å¯é€‰çš„åœºæ™¯æè¿°
 * }
 */
router.post('/start', async (req, res) => {
  try {
    const { topic, setting } = req.body;

    if (!topic) {
      return res.status(400).json({ error: 'topic is required' });
    }

    console.log(`ğŸ’¬ å¯åŠ¨å®æ—¶å¯¹è¯: ${topic}`);

    const systemInstruction = `You are a language learning scenario partner. Create realistic, educational dialogue scenarios.

Your role:
- Play the role of another character (vendor, waiter, friend, etc.) in the scenario
- Respond naturally to the student's choices
- Provide 3 response options for the student with different tones/appropriateness
- Keep conversations practical and educational

Response format:
- npc_says: What you (the NPC) say in this turn
- your_options: 3 options for how the student can respond
- Each option has: text, tone, appropriateness (excellent/good/acceptable/poor), brief_feedback

Keep it natural, conversational, and educational.`;

    const userPrompt = `Start a new dialogue scenario: "${topic}"

${setting ? `Setting: ${setting}` : ''}

This is the FIRST turn of the conversation. Set the scene and say the first line as the NPC character.

Provide 3 response options for the student with varying levels of appropriateness.`;

    const responseSchema = {
      type: Type.OBJECT,
      properties: {
        npc_says: { type: Type.STRING, description: "What the NPC says in this turn" },
        situation_context: { type: Type.STRING, description: "Brief context about the current situation" },
        your_options: {
          type: Type.ARRAY,
          items: {
            type: Type.OBJECT,
            properties: {
              text: { type: Type.STRING },
              tone: { type: Type.STRING },
              appropriateness: { 
                type: Type.STRING,
                enum: ['excellent', 'good', 'acceptable', 'poor']
              },
              brief_feedback: { type: Type.STRING, description: "1 sentence feedback" }
            }
          },
          minItems: 3,
          maxItems: 3
        }
      },
      required: ['npc_says', 'situation_context', 'your_options']
    };

    const response = await ai.models.generateContent({
      model: 'gemini-2.5-flash',
      contents: userPrompt,
      config: {
        systemInstruction,
        responseMimeType: 'application/json',
        responseSchema,
        temperature: 0.8,
        maxOutputTokens: 2048
      }
    });

    const dialogueTurn = JSON.parse(response.text);

    console.log('âœ… å¯¹è¯å¼€å§‹:', dialogueTurn.npc_says.substring(0, 50) + '...');

    res.json({
      turn: 1,
      ...dialogueTurn,
      conversation_history: []
    });

  } catch (error) {
    console.error('âŒ å¯åŠ¨å¯¹è¯å¤±è´¥:', error);
    res.status(500).json({ 
      error: 'å¯¹è¯å¯åŠ¨å¤±è´¥',
      details: (error as Error).message 
    });
  }
});

/**
 * POST /api/scenario/continue
 * ç»§ç»­å¯¹è¯ï¼ˆåŸºäºç”¨æˆ·çš„é€‰æ‹©ï¼‰
 * 
 * Body: {
 *   topic: string,
 *   conversation_history: Array<{npc_says: string, user_choice: string}>,
 *   user_choice_index: number  // ç”¨æˆ·é€‰æ‹©çš„é€‰é¡¹ç´¢å¼•
 *   last_options: Array<{text: string, tone: string}>
 * }
 */
router.post('/continue', async (req, res) => {
  try {
    const { topic, conversation_history, user_choice_index, last_options } = req.body;

    if (!topic || !last_options || user_choice_index === undefined) {
      return res.status(400).json({ error: 'Missing required parameters' });
    }

    const userChoice = last_options[user_choice_index];
    console.log(`ğŸ’¬ ç»§ç»­å¯¹è¯: ç”¨æˆ·é€‰æ‹©äº† [${userChoice.tone}] ${userChoice.text.substring(0, 30)}...`);

    const systemInstruction = `You are a language learning scenario partner. Continue the conversation naturally based on the student's choice.

Your role:
- React naturally to what the student said
- Keep the conversation flowing and educational
- Provide 3 new response options for the next turn
- Gradually progress the scenario toward a natural conclusion

Response format:
- npc_says: Your natural response to what the student just said
- your_options: 3 new options for the student's next response
- is_conversation_ending: true if this should be the last turn

Keep responses realistic and educational.`;

    // æ„å»ºå¯¹è¯å†å²ä¸Šä¸‹æ–‡
    let contextText = `Scenario: "${topic}"\n\nConversation so far:\n`;
    
    if (conversation_history && conversation_history.length > 0) {
      conversation_history.forEach((turn: any, i: number) => {
        contextText += `Turn ${i + 1}:\n`;
        contextText += `NPC: "${turn.npc_says}"\n`;
        contextText += `Student: "${turn.user_choice}"\n\n`;
      });
    }

    contextText += `Student just said: "${userChoice.text}" (tone: ${userChoice.tone})\n\n`;
    contextText += `Now generate the NPC's response and 3 new options for the student.`;

    const responseSchema = {
      type: Type.OBJECT,
      properties: {
        npc_says: { type: Type.STRING, description: "NPC's response to the student's choice" },
        situation_context: { type: Type.STRING, description: "Updated situation context" },
        your_options: {
          type: Type.ARRAY,
          items: {
            type: Type.OBJECT,
            properties: {
              text: { type: Type.STRING },
              tone: { type: Type.STRING },
              appropriateness: { 
                type: Type.STRING,
                enum: ['excellent', 'good', 'acceptable', 'poor']
              },
              brief_feedback: { type: Type.STRING, description: "1 sentence feedback" }
            }
          },
          minItems: 3,
          maxItems: 3
        },
        is_conversation_ending: { type: Type.BOOLEAN, description: "Should this be the last turn?" }
      },
      required: ['npc_says', 'situation_context', 'your_options']
    };

    const response = await ai.models.generateContent({
      model: 'gemini-2.5-flash',
      contents: contextText,
      config: {
        systemInstruction,
        responseMimeType: 'application/json',
        responseSchema,
        temperature: 0.8,
        maxOutputTokens: 2048
      }
    });

    const dialogueTurn = JSON.parse(response.text);

    console.log('âœ… å¯¹è¯ç»§ç»­:', dialogueTurn.npc_says.substring(0, 50) + '...');

    // æ›´æ–°å¯¹è¯å†å²
    const updatedHistory = [
      ...(conversation_history || []),
      {
        npc_says: conversation_history?.length > 0 ? conversation_history[conversation_history.length - 1].npc_says : '',
        user_choice: userChoice.text
      }
    ];

    res.json({
      turn: updatedHistory.length + 1,
      ...dialogueTurn,
      conversation_history: updatedHistory
    });

  } catch (error) {
    console.error('âŒ ç»§ç»­å¯¹è¯å¤±è´¥:', error);
    res.status(500).json({ 
      error: 'å¯¹è¯ç»§ç»­å¤±è´¥',
      details: (error as Error).message 
    });
  }
});

export default router;

